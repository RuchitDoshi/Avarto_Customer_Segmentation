{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries here; add more as necessary\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# magic word for producing visualizations in notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cmu/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (18,19) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "azdias = pd.read_csv('../capstone_data/Udacity_AZDIAS_052018.csv')\n",
    "# customers = pd.read_csv('../capstone_data/Udacity_CUSTOMERS_052018.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mailout_train=pd.read_csv('../capstone_data/Udacity_MAILOUT_052018_TRAIN.csv')\n",
    "# mailout_test=pd.read_csv('../capstone_data/Udacity_MAILOUT_052018_TEST.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprcocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Attribute_Unknown_Dict(path,df):\n",
    "    '''\n",
    "    Purpose: Map all the attributes to their unknown values in a dictionary\n",
    "    \n",
    "    Input: PATH TO DIAS Attributes - Values 2017.xlsx file, in str format\n",
    "    \n",
    "    Output:\n",
    "    new_dict:Dictionary with attributes as key and the unknown meaning values in a list\n",
    "    '''\n",
    "    #reading the xlsx file and storing it in a dataframe\n",
    "    attributes=pd.read_excel(path,skiprows=1)\n",
    "    \n",
    "    #formatting\n",
    "    attributes=attributes.drop(['Unnamed: 0'],axis=1,inplace=False)\n",
    "    \n",
    "    #Droping all the rows with nan values. Since, all the first values refer to the unknown meaning category\\\n",
    "    #Only keeping the first row for each attribute works\n",
    "    attributes=attributes.dropna()\n",
    "    \n",
    "    new_dict={}\n",
    "    for i in range(attributes.shape[0]):\n",
    "        #checking if it corresponds to the unknown value\n",
    "        if ('unknown' in attributes['Meaning'].iloc[i].split()) or(attributes['Meaning'].iloc[i] == 'no transaction known'):\n",
    "            new_list=[]\n",
    "            if type(attributes['Value'].iloc[i])==int:\n",
    "                new_list.append(attributes['Value'].iloc[i])\n",
    "            else:\n",
    "                for j in attributes['Value'].iloc[i].split(','):\n",
    "                    new_list.append(int(j))\n",
    "\n",
    "            new_dict[attributes['Attribute'].iloc[i]]=new_list\n",
    "    \n",
    "    #These columns are not present in  the actual dataset\n",
    "    new_dict.pop('BIP_FLAG')\n",
    "    \n",
    "    new_dict.pop('GEOSCORE_KLS7')\n",
    "    new_dict.pop('HAUSHALTSSTRUKTUR')\n",
    "    new_dict.pop('SOHO_FLAG')\n",
    "    new_dict.pop('WACHSTUMSGEBIET_NB')\n",
    "    \n",
    "    \n",
    "    #Removing name '_RZ' from the end of each column name\n",
    "    for i in new_dict:\n",
    "        if i not in df.columns:\n",
    "            new_dict[i[:-3]]=new_dict.pop(i)\n",
    "    \n",
    "    #removing the last key_value pair\n",
    "    new_dict.pop('')\n",
    "    \n",
    "    \n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapping the unknown values to nan with the help of the attribute_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacing unknown values with nan\n",
    "def Map_unknown_to_NAN(df,attribute_dict):\n",
    "    '''\n",
    "    Replace the unknown values with NAN values in the df\n",
    "    ARGS:\n",
    "    df: Dataframe on which the mapping takes place like azdias\n",
    "    attribute_dict: Dict with attribute as keys and values which are to replaced with NAN\n",
    "    \n",
    "    Output:\n",
    "    df: transformed df with more null values\n",
    "    \n",
    "    '''\n",
    "    #Replacing \n",
    "    for key,val in attribute_dict.items():\n",
    "        for j in val:\n",
    "            df[key]=df[key].replace(j,np.nan)\n",
    "            \n",
    "    #listing the columns which have more than 50% null values\n",
    "    drop_col_list=list(df.isnull().sum(axis=0)[df.isnull().sum(axis=0)>0.50*(df.shape[0])]\\\n",
    "                   .reset_index()['index'])\n",
    "    df.drop(drop_col_list,axis=1,inplace=True)\n",
    "    \n",
    "#     #listing the index of the rows which have more than 20% nan values and droping them\n",
    "#     drop_row_list=list(azdias.isnull().sum(axis=1)[azdias.isnull().sum(axis=1)>(0.2*azdias.shape[1])].reset_index()['index'])\n",
    "#     df=df.drop(index=drop_row_list)\n",
    "    \n",
    "                \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to replace these values with NAN value and convert CAMEO_INTL_2015 and CAMEO_DEUG_2015 into numerical entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_to_num(df):\n",
    "    '''\n",
    "    converting columns which are categorical to numerical and droping other categorical columns\n",
    "    INput: \n",
    "    df: DataFrame to be processed\n",
    "    \n",
    "    Output:\n",
    "    df: After droping and converting categorical columns\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #converting CAMEO_INTL_2015 AND CAMEO_DEUB_2015 into numerical values\n",
    "    intl=[]\n",
    "    deug=[]\n",
    "    for i in range(len(df['CAMEO_INTL_2015'])):\n",
    "        if type(df['CAMEO_INTL_2015'].iloc[i])==str and df['CAMEO_INTL_2015'].iloc[i][0]=='X':\n",
    "            intl.append(np.nan)\n",
    "        else:\n",
    "            intl.append(float(df['CAMEO_INTL_2015'].iloc[i]))\n",
    "        if type(df['CAMEO_INTL_2015'].iloc[i])==str and df['CAMEO_DEUG_2015'].iloc[i][0]=='X':\n",
    "            deug.append(np.nan)\n",
    "        else:\n",
    "            deug.append(float(df['CAMEO_DEUG_2015'].iloc[i]))\n",
    "    \n",
    "    #droping the original columns\n",
    "    df=df.drop(['CAMEO_INTL_2015','CAMEO_DEUG_2015'],axis=1,inplace=False)\n",
    "    #Adding new columns\n",
    "    df['CAMEO_INTL_2015']=intl\n",
    "    df['CAMEO_DEUG_2015']=deug\n",
    "    \n",
    "    #droping 'LNR' AND 'VERDICHTUNGSRAUM' columns\n",
    "    #droping the 'LP_FAMILIE_GROB' column because it very similar 'LP_FAMILIE_FEIN'\n",
    "\n",
    "    df=df.drop(['LNR','VERDICHTUNGSRAUM','LP_FAMILIE_GROB'],axis=1,inplace=False)\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mixed Categories can be segregated into individual categories. For example, 'CAMEO_INTL_2015' can be split into 'WEALTH' and 'LIFE_CYCLE'. Similarly, we can split 'PRAEGENDE_JUGENDJAHRE' into 'MOVEMENT' AND 'GENERATION'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixed_categories(df):\n",
    "    '''\n",
    "    Spliting mixed attributes into individua;\n",
    "    Input:\n",
    "    df: DataFrame to be processed\n",
    "    \n",
    "    Output:\n",
    "    df\n",
    "    '''\n",
    "    \n",
    "    df['WEALTH']=df['CAMEO_INTL_2015'].apply(lambda x:x/10)\n",
    "    df['LIFE_CYCLE']=df['CAMEO_INTL_2015'].apply(lambda x:x%10)\n",
    "    \n",
    "    mainstream=[1.0, 3.0, 5.0, 8.0, 10.0, 12.0, 14.0]\n",
    "    avantgarde=[2.0, 4.0, 6.0, 7.0, 9.0, 11.0, 13.0, 15.0]\n",
    "    \n",
    "    main=df['PRAEGENDE_JUGENDJAHRE'].isin([1.0, 3.0, 5.0, 8.0, 10.0, 12.0, 14.0])\n",
    "    avar=df['PRAEGENDE_JUGENDJAHRE'].isin([2.0, 4.0, 6.0, 7.0, 9.0, 11.0, 13.0, 15.0])\n",
    "    \n",
    "    df.loc[main,'MOVEMENT']=1.0\n",
    "    df.loc[avar,'MOVEMENT']=2.0\n",
    "    \n",
    "    df=df.drop(['CAMEO_INTL_2015','PRAEGENDE_JUGENDJAHRE', 'EINGEFUEGT_AM','D19_LETZTER_KAUF_BRANCHE'],axis=1,inplace=False)\n",
    "    \n",
    "    df['OST_WEST_KZ'] = df['OST_WEST_KZ'].replace({'O':1.0, 'W':2.0})\n",
    "    \n",
    "    new_df=pd.get_dummies(df,columns=['CAMEO_DEU_2015'])\n",
    "    \n",
    "    new_df=new_df.drop(['CAMEO_DEU_2015_XX'],axis=1,inplace=False)\n",
    "    \n",
    "    return new_df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_and_standardize(df):\n",
    "    '''\n",
    "    Replacing all the null values with mean of the column\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    fill_mean=lambda col:col.fillna(col.mean())\n",
    "    df=df.apply(fill_mean,axis=0)\n",
    "    \n",
    "    scaler=StandardScaler()\n",
    "    \n",
    "    df=scaler.fit_transform(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    '''\n",
    "    Function which integrates all the above functions and returns clean dataFrame\n",
    "    Input: data to be cleaned in dataFrame \n",
    "    \n",
    "    Output: clean data Frame\n",
    "    '''\n",
    "    \n",
    "    #Map all the attributes to their unknown values in a dictionary\n",
    "    attribute_dict=Attribute_Unknown_Dict('../capstone_data/DIAS Attributes - Values 2017.xlsx',df)\n",
    "    \n",
    "    df=Map_unknown_to_NAN(df,attribute_dict)\n",
    "    \n",
    "    df=cat_to_num(df)\n",
    "    \n",
    "    df=mixed_categories(df)\n",
    "    df_col=df.columns\n",
    "    \n",
    "    return df,df_col\n",
    "                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_azdias,azdias_col=clean_data(azdias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_azdias=impute_and_standardize(clean_azdias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del azdias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components=int((clean_azdias.shape[1])/1.8)\n",
    "\n",
    "pca = PCA(n_components)\n",
    "df_pca = pca.fit_transform(clean_azdias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=np.arange(n_components)\n",
    "value=pca.explained_variance_ratio_\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "ax = plt.subplot()\n",
    "cum_value = np.cumsum(value)\n",
    "ax.bar(index, value)\n",
    "ax.plot(index, cum_value)\n",
    "for i in range(n_components):\n",
    "    ax.xaxis.set_tick_params(width=0)\n",
    "    ax.yaxis.set_tick_params(width=3, length=12)\n",
    "\n",
    "    ax.set_xlabel(\"No of Principal Components\")\n",
    "    ax.set_ylabel(\"Variance (%)\")\n",
    "    plt.title('Variance Per Principal Component')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the PCA components to the features of the clean dataset. \n",
    "#Mapping each feature to the principal components\n",
    "\n",
    "dim =['Dimension' +str(i) for i in range(1,len(pca.components_)+1)]\n",
    "comp_dist=pd.DataFrame(np.round(pca.components_,5),columns=azdias_col)\n",
    "comp_dist.index=dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_dist.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_dist.iloc[0].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_dist.iloc[1].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_dist.iloc[3].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Azdias_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score=[]\n",
    "centers=np.arange(10,31)\n",
    "\n",
    "for i in centers:\n",
    "    \n",
    "    k_mean=MiniBatchKMeans(n_clusters=i).fit(df_pca)\n",
    "    #Append the score with different centers\n",
    "    score.append(np.abs(k_mean.score(df_pca)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(centers,score,marker='^')\n",
    "plt.xlabel('No of centers')\n",
    "plt.ylabel('SSE')\n",
    "plt.title('Sum of Squared distance for different no of centers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans=KMeans(n_clusters=30)\n",
    "df_kmeans=kmeans.fit_predict(df_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster=pd.DataFrame(np.round(df_pca,5),columns=dim)\n",
    "df_cluster.insert(loc=0,column='Cluster',value=df_kmeans)\n",
    "df_cluster.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying these changes to the customer_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = pd.read_csv('../capstone_data/Udacity_CUSTOMERS_052018.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers=customers.drop(['CUSTOMER_GROUP', 'ONLINE_PURCHASE', 'PRODUCT_GROUP'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data_cust(df):\n",
    "    '''\n",
    "    Function which integrates all the above functions and returns clean dataFrame\n",
    "    Input: data to be cleaned in dataFrame \n",
    "    \n",
    "    Output: clean data Frame\n",
    "    '''\n",
    "    \n",
    "    #Map all the attributes to their unknown values in a dictionary\n",
    "    attribute_dict=Attribute_Unknown_Dict('../capstone_data/DIAS Attributes - Values 2017.xlsx',df)\n",
    "    \n",
    "    df=Map_unknown_to_NAN(df,attribute_dict)\n",
    "    \n",
    "    df=cat_to_num(df)\n",
    "    \n",
    "    df=mixed_categories(df)\n",
    "    df_col=df.columns    \n",
    "    \n",
    "    return df,df_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_customer,cust_col=clean_data_cust(customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_customer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the column diff between customer and azdias df\n",
    "missing = list(np.setdiff1d(cust_col, azdias_col))\n",
    "clean_customer=clean_customer.drop(columns=missing,axis=1,inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_cust=impute_and_standardize(clean_customer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_pca=pca.transform(clean_cust)\n",
    "cust_kmeans=kmeans.predict(cust_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_df=pd.DataFrame(np.round(cust_pca,5),columns=dim)\n",
    "cust_df.insert(loc=0,column='Cluster',value=cust_kmeans)\n",
    "cust_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(cust_df['Cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(df_cluster['Cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a df from the difference of both the clusters\n",
    "azdias_count=(df_cluster['Cluster'].value_counts()/df_cluster.shape[0])\n",
    "cust_count=(cust_df['Cluster'].value_counts()/cust_df.shape[0])\n",
    "\n",
    "diff_count=cust_count-azdias_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_count=diff_count.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_count.plot.barh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_cluster_to_columns(cluster_num):\n",
    "    '''\n",
    "    Function to infer the columns which are similar or which are dissimilar\n",
    "    Input:\n",
    "    cluster_num: Cluster_number\n",
    "    '''\n",
    "    temp=cust_df[cust_df['Cluster']==cluster_num]\n",
    "    temp=temp.drop(['Cluster'],axis=1,inplace=False)\n",
    "    \n",
    "    cluster_pca=pca.inverse_transform(temp)\n",
    "    \n",
    "    cluster_final=pd.DataFrame(cluster_pca,columns=clean_customer.columns)\n",
    "    \n",
    "    return cluster_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target=map_cluster_to_columns(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
